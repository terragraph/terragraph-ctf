<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-tests/QoS">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">QoS Tests | Terragraph CTF</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://terragraph.github.io/terragraph-ctf/logo/terragraph-logo-full-RGB.png"><meta data-rh="true" name="twitter:image" content="https://terragraph.github.io/terragraph-ctf/logo/terragraph-logo-full-RGB.png"><meta data-rh="true" property="og:url" content="https://terragraph.github.io/terragraph-ctf/tests/QoS"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="QoS Tests | Terragraph CTF"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="icon" href="/terragraph-ctf/logo/terragraph-logo-favicon-32x32-full-RGB.png"><link data-rh="true" rel="canonical" href="https://terragraph.github.io/terragraph-ctf/tests/QoS"><link data-rh="true" rel="alternate" href="https://terragraph.github.io/terragraph-ctf/tests/QoS" hreflang="en"><link data-rh="true" rel="alternate" href="https://terragraph.github.io/terragraph-ctf/tests/QoS" hreflang="x-default"><link rel="stylesheet" href="/terragraph-ctf/katex/katex.min.css"><link rel="stylesheet" href="/terragraph-ctf/assets/css/styles.273bd273.css">
<link rel="preload" href="/terragraph-ctf/assets/js/runtime~main.4f2908c1.js" as="script">
<link rel="preload" href="/terragraph-ctf/assets/js/main.8da3f691.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/terragraph-ctf/"><div class="navbar__logo"><img src="/terragraph-ctf/logo/terragraph-logo-favicon-32x32-full-RGB.svg" alt="Terragraph Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/terragraph-ctf/logo/terragraph-logo-favicon-32x32-white-RGB.svg" alt="Terragraph Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/terragraph/terragraph-ctf" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link githubButton navbarIconButton" title="GitHub"></a><a href="https://discord.gg/HQaxCevzus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link discordButton navbarIconButton" title="Discord"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/terragraph-ctf/">Terragraph Test Plans</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/terragraph-ctf/tests/IBF">Tests</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/IBF">IBF Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Association">Association Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Throughput">Throughput Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Link_Adaptation">Link Adaptation Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Interference">Interference Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/802.1X">802.1X Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/WSEC">WSEC (WPA-PSK) Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/LLS">Link Level Scheduler Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Scans">Scan Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/SW_Hybrid">SW Hybrid Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Y_Street">Y-Street Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Z_Street">Z-Street Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Coordinated_Scheduling">Coordinated Scheduling Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/E2E_Controller">E2E Controller Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Stability">Stability Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/RFC_2544">RFC 2544 Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Routing">Routing Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Prefix_Allocation">Prefix Allocation (CPA/DPA) Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/terragraph-ctf/tests/QoS">QoS Tests</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/terragraph-ctf/tests/Link_Overloading">Link Overloading Tests</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/terragraph-ctf/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Tests</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">QoS Tests</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>QoS Tests</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>Consistent with the QoS requirements partially specified by the Puma PRD and by
our data path design, a Terragraph DN must support the following:</p><ol><li>Uplink traffic policing performed separately for each of 4 Traffic Classes:<ol><li>Traffic marked to be within committed rate (CIR)</li><li>Traffic marked to be in excess of committed rate (EIR) but below peak
rate</li><li>Traffic above peak rate shall be dropped</li><li>CIR and EIR can be configured with different values for different Traffic
Classes</li></ol></li><li>Downlink traffic policing is expected to be done external to the Terragraph
network.</li><li>After policing, the Traffic Class and Drop Preference are indicated by the
IPv6 header DSCP value. The mapping from Traffic Class and Drop Preference to
DSCP value is configurable and identical for downlink and uplink traffic.</li><li>[Not specified in PRD but consistent with the implementation]<!-- --> At each TG
sector, the 4 Traffic Classes are served according to a strict priority
policy — in this document, TC0 is the highest priority and TC3 is the lowest
priority.</li><li>[Not specified clearly]<!-- --> Uplink traffic policing is required for ingress on
the 1G interface for both DN and CN operation. It is not required to support
uplink traffic policing for ingress on the DN 10G interface. It is not yet
determined whether the implementation will support policing on the 10G
interface; this test plan is defined assuming not.</li><li>[Not specific to QoS]<!-- --> According to the finalized Puma HW design, it is
expected that a single Puma DN can simultaneously sustain MCS 12 data rates
on 2 specific sectors and MCS 11 data rates on the 2 other specific sectors.
Additionally and specifically, the system will constrain the two relevant
sectors to a maximum of MCS 11 at all times.</li></ol><p>The point of this test plan is to confirm the proper functionality of the QoS
functions, which requires more elaborate traffic scenarios than many other types
of validation tests. Many of the pass/fail criteria in this document include &quot;~X
Mbps&quot; to indicate &quot;about X Mbps,&quot; based on what we should expect. The criteria
here are intentionally not specified to precisely account for various protocol
and control traffic overheads -- that is not the intention for these acceptance
tests.</p><p>Note: All of the test setups in this document assume IF connections between TG
sectors. If only RF connections are possible, it is fine to use RF connections
instead provided they can reliably support the expected PHY rates for each test
(MCS 9, MCS 12, or as specified in the test).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-plane-operation-and-traffic-models">Data Plane Operation and Traffic Models<a class="hash-link" href="#data-plane-operation-and-traffic-models" title="Direct link to heading">​</a></h2><p>Validation tests are based on two traffic models: IMIX and fixed rate traffic.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="user-plane-traffic">User Plane Traffic<a class="hash-link" href="#user-plane-traffic" title="Direct link to heading">​</a></h3><p>User plane data traffic is generated using the following traffic models.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="traffic-with-fixed-packet-size">Traffic with Fixed Packet Size<a class="hash-link" href="#traffic-with-fixed-packet-size" title="Direct link to heading">​</a></h4><p>In this traffic model, packet size will be fixed at 128 bytes or greater, up to
1500 bytes. Any fixed packet size between 128 and 1500 bytes, inclusive, is
allowed. Packet interarrivals may be constant or exponential (i.e. Poisson
arrivals). In both cases the arrival rate will be chosen to give the desired
data rate in Gbps or Mbps. Note: iPerf may be used to generate this traffic with
a fixed arrival rate.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="imix-traffic-models">IMIX Traffic Models<a class="hash-link" href="#imix-traffic-models" title="Direct link to heading">​</a></h4><p>The following table contains the distribution of IMIX data used for acceptance:</p><table><thead><tr><th>Packet Size in Bytes (incl. IP Header)</th><th>Distribution (in packets)</th></tr></thead><tbody><tr><td>40</td><td>58.3333%</td></tr><tr><td>576</td><td>33.3333%</td></tr><tr><td>1500</td><td>8.3333%</td></tr></tbody></table><p>With an average packet size of 340 bytes, the packet rate for IMIX traffic to
achieve 2 Gbps, for example, is approximately ~735Kpps.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="imix-generation-model-1">IMIX Generation Model 1<a class="hash-link" href="#imix-generation-model-1" title="Direct link to heading">​</a></h4><p>A hardware generator such as Ixia can be used to generate packets with a Poisson
arrival rate. The size of every arriving packet is random and is either 40, 576,
or 1500 bytes with the size given by the probability distribution in the table
above. The arrival rate is chosen to give the desire data rate in Gbps or Mbps.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="imix-generation-model-2">IMIX Generation Model 2<a class="hash-link" href="#imix-generation-model-2" title="Direct link to heading">​</a></h4><p>In this model we mix 3 streams of constant size packets (given in the table
above). The ratios of the data rates for each stream is chosen to obtain the
IMIX distribution. The absolute rate of each stream is chosen to give the
overall desired rate. For example, to achieve a rate of approximately 2.5 Gbps,
we can generate three data flows with the following settings:</p><ol><li>For flow 1, generate packets at rate 535.6 KPPS with packet size 40 bytes</li><li>For flow 2, generate packets at rate 306.1 KPPS with packet size 576 bytes</li><li>For flow 3, generate packets at rate 76.5 KPPS with packet size 1500 bytes</li></ol><p>Note that the constant packet model does not test any burstiness brought by the
IMIX model.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="traffic-class-identification-for-user-plane-traffic">Traffic Class Identification for User Plane Traffic<a class="hash-link" href="#traffic-class-identification-for-user-plane-traffic" title="Direct link to heading">​</a></h4><p>The requirement is to support strict priority between the Traffic Classes -- we
denote by TC0 the highest priority. With 4 Traffic Classes, TC3 denotes the
lowest priority.</p><p>The classes are identified by the DSCP value in the IPv6 header. Additionally,
rate policing is signaled using two different DSCP values for each Traffic
Class, one signifying &#x27;committed&#x27; traffic and the other &#x27;excess&#x27; traffic (higher
drop preference). The mapping from DSCP to Traffic Class and Drop Preference
must be configurable. For testing, we use the following mapping from Traffic
Class and Drop Preference to DSCP:</p><table><thead><tr><th>QoS Class, Drop Preference</th><th>TC0, Low (Green)</th><th>TC0, High (Yellow)</th><th>TC1, Low (Green)</th><th>TC1, High (Yellow)</th><th>TC2, Low (Green)</th><th>TC2, High (Yellow)</th><th>TC3, Low (Green)</th><th>TC3, High (Yellow)</th></tr></thead><tbody><tr><td>DSCP Value (Decimal)</td><td>34</td><td>36</td><td>26</td><td>28</td><td>18</td><td>20</td><td>10</td><td>12</td></tr></tbody></table><p>Note: Logically this mapping can be arbitrary. This mapping is based on commonly
used DiffServ mappings, with:</p><ul><li>(TC0, Green / Yellow) corresponding to &#x27;AF41&#x27; /&#x27;AF42&#x27;</li><li>(TC1, Green / Yellow) corresponding to &#x27;AF31&#x27; / &#x27;AF32&#x27;</li><li>(TC2, Green / Yellow) corresponding to &#x27;AF21&#x27; / &#x27;AF22&#x27;</li><li>(TC3, Green / Yellow) corresponding to &#x27;AF11&#x27; / &#x27;AF12&#x27;</li></ul><p>For traffic beyond a policer, all other DSCP values should be handled
equivalently to TC3, Yellow -- i.e. the lowest priority. Ideally this will never
happen, but the implementation should handle it gracefully.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="policing">Policing<a class="hash-link" href="#policing" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-rate--3-color-policing">2 rate / 3 color policing<a class="hash-link" href="#2-rate--3-color-policing" title="Direct link to heading">​</a></h4><p>According to QoS requirements, policing is done separately per Traffic Class. By
definition, the policing function is a 2 rate / 3 color policer. Traffic
conforming to the lower rate is considered &#x27;green&#x27;, corresponding to low drop
preference (last to be dropped from that Traffic Class). Traffic beyond &#x27;green&#x27;
and conforming to the higher rate is considered &#x27;yellow&#x27;, corresponding to
higher drop preference (dropped before &#x27;green&#x27; traffic). Traffic beyond &#x27;green&#x27;
and &#x27;yellow&#x27; is considered &#x27;red&#x27; and must not be forwarded to another TG sector.</p><p>Traffic inbound to a policing function that has an unknown DSCP value must be
considered equivalent to TC3, Green.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="upstream-versus-downstream-policing">Upstream versus downstream policing<a class="hash-link" href="#upstream-versus-downstream-policing" title="Direct link to heading">​</a></h4><p>Traffic inbound from a user device attached to a TG node is considered upstream
traffic, and this is required only on a 1G interface / port. Traffic inbound to
the TG network from a POP node toward a user device is considered downstream
traffic and is expected on a 10G interface / port.</p><p>Upstream traffic must be policed on the TG node to which it is attached.
Policing corresponds to applying the 2 rate / 3 color policing function
configured for that user device and Traffic Class. At ingress to the TG node,
the upstream traffic is expected to be marked with DSCP corresponding to one of
the supported Traffic Classes with low drop preference (Green). Depending on the
inbound rate and corresponding policing function configuration, some traffic
will pass unmodified, some traffic potentially will have DSCP remarked to the
corresponding &#x27;Yellow&#x27; color of that Traffic Class, and some traffic potentially
dropped within the node due to exceeding the second configured policing function
rate, i.e. being &#x27;Red&#x27;.</p><p>Downstream traffic is not policed directly within the TG (DN POP) node. The
policing function is instead applied within the core network edge router (or
before). It is expected to be properly marked by DSCP according to one of the
supported Traffic Classes and Drop Preferences. In this test plan, we assume the
traffic generators do not have internal 2 rate / 3 color policing functionality
but can generate traffic with different specified DSCPs. Therefore, for
downstream traffic of a particular Traffic Class, we generate downstream &#x27;Green&#x27;
and &#x27;Yellow&#x27; traffic independently and in parallel. This prevents us from easily
testing any downstream scenario with congestion for a Traffic Class carrying TCP
traffic -- the traffic source will not be able to produce a single TCP stream
with a mix of Green and Yellow traffic.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="testing-defaults-mostly-configuration">Testing Defaults (Mostly Configuration)<a class="hash-link" href="#testing-defaults-mostly-configuration" title="Direct link to heading">​</a></h2><p>Unless otherwise specified in a particular test:</p><ol><li>UDP traffic with fixed size packets of size 512 bytes, and fixed packet
inter-arrivals determined by the offered traffic throughput (TCP traffic will
be used heavily for general testing but limited for QoS tests).<ol><li>There is a known core data plane packet processing limitation of
approximately 450kPPS for traffic routed from WiGig to WiGig interfaces.
Until this known PPS limitation is improved, we should apply the
following:<ol><li>1150B payload size (or larger) for multi-hop bidirectional tests</li><li>800B payload size (or larger) for multi-hop unidirectional tests</li><li>These should work for multihop tests where TG nodes have at most 2
sectors handling traffic, and should work for single hop tests with
yet more headroom.</li></ol></li></ol></li><li>When not using precision test equipment to characterize delay, <code>iperf3</code>
should be used.</li><li>Upstream policing function for each Traffic Class:<ol><li>Configured as <a href="https://tools.ietf.org/html/rfc4115" target="_blank" rel="noopener noreferrer">RFC 4115</a>, which
includes the following configuration parameters:<ol><li>Green / CIR = 300 Mbps.</li><li>Yellow / EIR = 700 Mbps.</li><li>CBS = TBD (may have to tune during testing). This parameter is
&#x27;Committed Burst Size&#x27;, defining the token bucket size (in bytes)
used for marking CIR traffic.</li><li>EBS = TBD (may have to tune during testing). This is &#x27;Excess Burst
Size&#x27;, defining the token bucket size (in bytes) used for marking EIR
traffic.</li></ol></li></ol></li><li>Test traffic run for 180 seconds. When verifying throughputs, ignore the
first 5 seconds after traffic is started so that we measure the performance
in steady state, consistent with how we typically test and verify Terragraph
system performance (specifically, ignore traffic for the same amount of time
as we typically do for testing, if not 5 seconds).</li><li>All tests in this test plan fix MCS in order to provide predictable pass/fail
criteria for the different traffic class streams. Transmit power shall also
be fixed (appropriate for the physical setup) to avoid introducing PER.</li><li>Some of these tests are based on MCS = 6. We originally expected this MCS to
yield approximately 750 Mbps bidirectional throughput for UDP. Based on the
extra hugging and the changes in throughput from Qualcomm, the expected MCS 6
rate is 587 Mbps for UDP. To confirm, we measured 602 Mbps in the lab.
Therefore we modified all tests based on MCS = 6 in this document to assume
590 Mbps bidirectional throughput for UDP.</li><li>Average packet delay, average delay jitter, and histogram of packet delay
should be captured for each separate traffic flow as part of test results, if
the test equipment permits. We will need to make an engineering judgment call
whether the results are acceptable.</li><li>Many tests indicate “verify ~ X Mbps&quot; for a traffic flow. To be precise,
define the pass / fail criterion as at least 90% of X Mbps.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="vpp-policer-configuration">VPP policer configuration:<a class="hash-link" href="#vpp-policer-configuration" title="Direct link to heading">​</a></h3><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">/usr/bin/tg_vpp_cfg -c /etc/vpp/vpp_config.json -t -i \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -p policer_add_del_tc0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -p policer_add_del_tc1 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -p policer_add_del_tc2 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -p policer_add_del_tc3 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -s classify_add_del_session_tc0_af4 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -s classify_add_del_session_tc1_af3 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -s classify_add_del_session_tc2_af2 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -s classify_add_del_session_tc3_af1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The default config is stored in <code>/etc/vpp/vpp_config.json</code> and the above command
will apply the QoS policer config stored in the JSON. Please note that PIR
should be EIR + CIR (= PIR) and you should expect a receive rate of PIR.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="important-note-for-all-test-cases">Important Note for All Test Cases<a class="hash-link" href="#important-note-for-all-test-cases" title="Direct link to heading">​</a></h3><p>This test plan assumes WRED is implemented for queue congestion management,
which allows us to distinguish between CIR vs. EIR traffic during congestion.
This will not meet our end requirements, but it will have a lot of new
functionality that we should test and debug — max rate limiting per Traffic
Class to CIR+EIR, and Strict Priority scheduling. For that interim
implementation, we can modify every test case by changing &#x27;Verify on receive&#x27; —
specifically, change to verify only the sum of the specified expected Green +
Yellow (separately for each Traffic Class). For example, if a test indicates:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN0 to CN, TC1 Green</td><td>100</td><td>100, no packet loss</td></tr><tr><td>DN0 to CN, TC1 Yellow</td><td>175</td><td>175, no packet loss</td></tr><tr><td>DN0 to CN, TC2 Green</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN, TC2 Yellow</td><td>125</td><td>~50</td></tr></tbody></table><p>For the interim tail drop implementation, we would modify to:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN0 to CN, TC1 Green</td><td>100</td><td>275 Green + Yellow, no packet loss</td></tr><tr><td>DN0 to CN, TC1 Yellow</td><td>175</td><td>275 Green + Yellow, no packet loss</td></tr><tr><td>DN0 to CN, TC2 Green</td><td>150</td><td>~200 Green + Yellow</td></tr><tr><td>DN0 to CN, TC2 Yellow</td><td>125</td><td>~200 Green + Yellow</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="point-to-point-test-cases---p2p1">Point to Point Test Cases - P2P1<a class="hash-link" href="#point-to-point-test-cases---p2p1" title="Direct link to heading">​</a></h2><p>For some Point to Point tests, we use the following IF setup, denoted <strong>P2P1</strong>:</p><p align="center"><img loading="lazy" src="/terragraph-ctf/figures/QoS-P2P1.png" width="800" class="img_ev3q"></p><p>The connection between DN0 and CN/DN1 is an IF connection. In this diagram &#x27;CN&#x27;
means Puma configured to operate as a CN. &#x27;DN1&#x27; means Puma configured to operate
as a DN.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-1-basic-policer-operation----enforce-cir--eir"><code>QoS-P2P-1</code> Basic policer operation -- enforce CIR + EIR<a class="hash-link" href="#qos-p2p-1-basic-policer-operation----enforce-cir--eir" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix lamaxMCS = 9, yielding approximately 1 Gbps bidirectional throughput on
that link</li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>TC0: 50 Mbps / 150 Mbps</li><li>TC3: 150 Mbps / 150 Mbps</li></ul><p>Note on iPerf:</p><ul><li><code>iperf -S 0x88</code> → TOS = 0x88 → AF41 → TC0</li><li><code>iperf -S 0x28</code> → TOS = 0x28 → AF11 → TC3</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>300</td><td>~200, Green + Yellow</td></tr><tr><td>CN to DN0, TC3 Green</td><td>500</td><td>~300, Green + Yellow</td></tr></tbody></table><p>Notes:</p><ul><li>From stats: Verify dropped packet counters (on CN, due to policing function)
appear to increase at the appropriate rate. Verify similarly for TC3 traffic.</li></ul><p><strong>Repeat the test with CN/DN1 node configured to operate as a DN (with the same
policer configuration for DN1).</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-2-characterize-basic-delay-measurements"><code>QoS-P2P-2</code> Characterize basic delay measurements<a class="hash-link" href="#qos-p2p-2-characterize-basic-delay-measurements" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix lamaxMCS = 12, yielding approximately 1.8 Gbps bidirectional throughput on
that link<ul><li>Note: ensure we are using Puma sectors that can support MCS = 12.</li></ul></li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>Default (irrelevant -- not exercised in this test)</li></ul><p><strong>Using precision test equipment</strong> (e.g., Ixia), traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr><tr><td>CN to DN0, TC3 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr><tr><td>DN0 to CN, TC0 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr><tr><td>DN0 to CN, TC3 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr></tbody></table><p>Notes:</p><ul><li>For this test, we will need to make an engineering judgment call whether the
absolute 1-way latency average and standard deviations are acceptable.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-3-characterize-priority-traffic-delay-with-load"><code>QoS-P2P-3</code> Characterize priority traffic delay with load<a class="hash-link" href="#qos-p2p-3-characterize-priority-traffic-delay-with-load" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 12, yielding approximately 1.8 Gbps bidirectional throughput on that link<ul><li>Note: ensure we are using Puma sectors that can support MCS = 12.</li></ul></li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>Default (irrelevant -- not exercised in this test)</li></ul><p><strong>Using precision test equipment</strong> (e.g., Ixia), traffic simultaneous send /
verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr><tr><td>CN to DN0, TC3 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr><tr><td>DN0 to CN, TC0 Green</td><td>100 kbs with 100B packets</td><td>No packet loss, average 1-way latency &lt; 1 msec</td></tr><tr><td>DN0 to CN, TC3 Green</td><td>2 Gbps with 1500B packets</td><td>~ 1.8 Gbps</td></tr></tbody></table><p>Notes:</p><ul><li>For this test, we will need to make an engineering judgment call whether the
absolute 1-way latency average and standard deviations are acceptable.</li><li>Given the current data plane architecture and configuration, we should expect
that average 1-way latency for DN0 to CN, TC0 Green should expect &lt; 2 msec
latency (rather than 1 msec). With this traffic profile, 1-way latency in the
opposite direction (CN to DN0) should be low. Therefore if we use ping to
measure round-trip latency rather than Ixia, we should pass if TC0 ping RTT &lt;
2.5 msec.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-4-basic-priority-service-with-congestion-response"><code>QoS-P2P-4</code> Basic priority service with congestion response<a class="hash-link" href="#qos-p2p-4-basic-priority-service-with-congestion-response" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 6, yielding approximately 590 Mbps bidirectional throughput (see
note in Testing Defaults)</li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>TC0: 50 Mbps / 950 Mbps</li><li>TC3: 300 Mbps / 700 Mbps</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>200</td><td>200, Green +  Yellow, no packet loss<br>(50 Mbps Green; 150 Mbps Yellow)</td></tr><tr><td>CN to DN0, TC3 Green</td><td>700</td><td>~ 390, Green + Yellow<br>Note: OK to be ~12 Mbps higher due to MCS = 6 getting ~602 Mbps (measured) rather than 590 Mbps (predicted)</td></tr><tr><td>DN0 to CN, TC0 Green</td><td>50</td><td>50, no packet loss</td></tr><tr><td>DN0 to CN, TC0 Yellow</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN, TC3 Green</td><td>300</td><td>300, no packet loss</td></tr><tr><td>DN0 to CN, TC3 Yellow</td><td>500 Mbps</td><td>~ 90<br>Note: OK to be ~12 Mbps higher due to MCS = 6 getting ~602 Mbps (measured) rather than 590 Mbps (predicted)</td></tr></tbody></table><p>Notes:</p><ul><li>From stats, verify dropped packet counters for TC3 on both CN and DN0 (due to
congestion).</li><li>Repeat the test with DN0 to CN, TC3 Yellow of 2000 Mbps rather than 500 Mbps —
same expected throughput to verify.</li></ul><p><strong>Repeat with CN/DN1 node configured to operate as a DN (with the same policer
configuration for DN1).</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-5-basic-priority-service-with-time-varying-congestion-response"><code>QoS-P2P-5</code> Basic priority service with time-varying congestion response<a class="hash-link" href="#qos-p2p-5-basic-priority-service-with-time-varying-congestion-response" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 6, yielding approximately 590 Mbps bidirectional throughput (see
note in Testing Defaults)</li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>Default (irrelevant -- not exercised in this test)</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN0 to CN, TC0 Green</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN, TC3 Green</td><td>300</td><td>300, no packet loss</td></tr><tr><td>DN0 to CN, TC3 Yellow</td><td>Repeat 5 iterations:<br>1. 100 for 30 seconds, then<br>2. 550 for 30 seconds</td><td>100 when sending 100, and ~140 when sending 550<br>Note: OK to be ~12 Mbps higher due to MCS = 6 getting ~602 Mbps (measured) rather than 590 Mbps (predicted)</td></tr></tbody></table><p>Notes:</p><ul><li>The intention of this test is to verify that the queue drop policy responds
properly to the change in congestion state for a particular Traffic Class, and
the other Traffic Class traffic is not affected in any way.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qop2p-6-policer-remarking-with-tcp----confirm-packet-order-preserved"><code>QoP2P-6</code> Policer remarking with TCP -- confirm packet order preserved<a class="hash-link" href="#qop2p-6-policer-remarking-with-tcp----confirm-packet-order-preserved" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 9, yielding approximately 1 Gbps bidirectional throughput</li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>TC0: 50 Mbps / 950 Mbps</li><li>TC3: 300 Mbps / 700 Gbps</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>200 TCP</td><td>200, Green + Yellow</td></tr><tr><td>CN to DN0, TC3 Green</td><td>700 TCP</td><td>700, Green + Yellow</td></tr></tbody></table><p><strong>Repeat with CN/DN1 node configured to operate as a DN (with the same policer
configuration for DN1).</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-7-four-traffic-classes-with-strict-priority-service"><code>QoS-P2P-7</code> Four traffic classes with strict priority service<a class="hash-link" href="#qos-p2p-7-four-traffic-classes-with-strict-priority-service" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 6, yielding approximately 590 Mbps bidirectional throughput (see
note in Testing Defaults)</li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>TC0: 50 Mbps / 400 Mbps</li><li>TC1: 100 Mbps / 400 Mbps</li><li>TC2: 150 Mbps / 400 Mbps</li><li>TC3: 200 Mbps / 400 Mbps</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>225</td><td>225, no packet loss</td></tr><tr><td>CN to DN0, TC1 Green</td><td>225</td><td>225, no packet loss</td></tr><tr><td>CN to DN0, TC2 Green</td><td>225</td><td>~140<br>Note: OK to be ~12 Mbps higher due to MCS = 6 getting ~602 Mbps (measured) rather than 590 Mbps (predicted)</td></tr><tr><td>CN to DN0, TC3 Green</td><td>75</td><td>~0</td></tr><tr><td>DN0 to CN, TC0 Green</td><td>50</td><td>50, no packet loss</td></tr><tr><td>DN0 to CN, TC0 Yellow</td><td>100</td><td>100, no packet loss</td></tr><tr><td>DN0 to CN, TC1 Green</td><td>100</td><td>100, no packet loss</td></tr><tr><td>DN0 to CN, TC1 Yellow</td><td>140</td><td>140, no packet loss</td></tr><tr><td>DN0 to CN, TC2 Green</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN, TC2 Yellow</td><td>125</td><td>~50<br>Note: OK to be ~12 Mbps higher due to MCS = 6 getting ~602 Mbps (measured) rather than 590 Mbps (predicted)</td></tr><tr><td>DN0 to CN, TC3 Green</td><td>170</td><td>0</td></tr><tr><td>DN0 to CN, TC3 Yellow</td><td>0<br>(just for completeness -- 0 is correct)</td><td>0</td></tr></tbody></table><p><strong>Repeat with CN/DN1 node configured to operate as a DN (with the same policer
configuration for DN1).</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-8-handling-unknown-dscps"><code>QoS-P2P-8</code> Handling unknown DSCPs<a class="hash-link" href="#qos-p2p-8-handling-unknown-dscps" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 9, yielding approximately 1 Gbps bidirectional throughput on that
link</li></ul><p>CN/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>TC0: 50 Mbps / 150 Mbps</li><li>TC3: 150 Mbps / 350 Mbps</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN to DN0, TC0 Green</td><td>300</td><td>200, Green + Yellow</td></tr><tr><td>CN to DN0, TC3 Green</td><td>300</td><td>500, Green + Yellow</td></tr><tr><td>CN to DN0, Unknown DSCP (use DSCP = 17)</td><td>300</td><td>500, Green + Yellow</td></tr><tr><td>DN0 to CN, TC0 Green</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN, TC0 Yellow</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN, TC3 Green</td><td>300</td><td>300, no packet loss</td></tr><tr><td>DN0 to CN, TC3 Yellow</td><td>500</td><td>~ 200</td></tr><tr><td>DN0 to CN, Unknown DSCP (use DSCP = 17)</td><td>500</td><td>~ 200</td></tr></tbody></table><p>Notes:</p><ul><li>For downstream traffic, an unknown DSCP is expected to be handled equivalently
to lowest priority traffic with high drop preference — i.e. TC3 Yellow
traffic. The DSCP value is not expected to be changed.</li><li>For upstream traffic, an unknown DSCP is expected to never occur once the
transmit path Network Integration Function is in place (one aspect is the
responsibility to set the DSCP to one of the expected values). Regardless,
should it occur, the implementation is expected to handle this equivalently to
lowest priority traffic with low drop preference — i.e. TC3 Green traffic —
which is then subject to the TC3 policing function. The DSCP value is expected
to be changed.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="point-to-point-test-cases---p2p2">Point to Point Test Cases - P2P2<a class="hash-link" href="#point-to-point-test-cases---p2p2" title="Direct link to heading">​</a></h2><p>For some Point to Point tests, we use the following IF setup, denoted <strong>P2P2</strong>:</p><p align="center"><img loading="lazy" src="/terragraph-ctf/figures/QoS-P2P2.png" width="800" class="img_ev3q"></p><p>The connection between DN0 and DN1 is an IF connection. Note the only difference
between P2P1 and P2P2 is that P2P2 has two DN sectors, each with the external
traffic source connected to its 10G interface.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2p-9-single-link-performance-test"><code>QoS-P2P-9</code> Single link performance test<a class="hash-link" href="#qos-p2p-9-single-link-performance-test" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2P2</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 12, yielding approximately 1.8 Gbps bidirectional throughput<ul><li>Note: ensure we are using Puma sectors that can support MCS = 12.</li></ul></li></ul><p>DN policer configuration:</p><ul><li>N/A (irrelevant -- no end devices attached to GigE ports)</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN0 to DN1, TC3 Green</td><td>1800 with fixed 1500B packets</td><td>1800, no packet loss</td></tr><tr><td>DN1 to DN0, TC3 Green</td><td>1800 with fixed 1500B packets</td><td>1800, no packet loss</td></tr></tbody></table><ol><li>Repeat with TCP at max rate 1800 Mbps and verify ~ 1800 Mbps in each
direction.</li><li>Repeat with TC3 Green TCP at max rate 1500 Mbps + TC1 Green UDP at rate 100
Mbps. Verify aggregate ~ 1600 Mbps each direction and no UDP packet loss.<ol><li>Note: we use an aggregate 1600 Mbps to ensure the TC3 queue does not back
up and drop TCP packets, which would cause TCP to back off. The pass /
fail criteria would be less clear in this case.</li></ol></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="point-to-multipoint-test-cases---p2mp">Point to Multipoint Test Cases - P2MP<a class="hash-link" href="#point-to-multipoint-test-cases---p2mp" title="Direct link to heading">​</a></h2><p>For Point to Multipoint tests, we use the following IF setup, denoted <strong>P2MP</strong>:</p><p align="center"><img loading="lazy" src="/terragraph-ctf/figures/QoS-P2MP.png" width="800" class="img_ev3q"></p><p>DN0 has a single sector connected to the two CNs, or 1 CN and 1 DN, in a P2MP
configuration.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2mp-1-congestion-control-per-link-per-tc"><code>QoS-P2MP-1</code> Congestion control (per link, per TC)<a class="hash-link" href="#qos-p2mp-1-congestion-control-per-link-per-tc" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2MP</strong></p><p>Channel configuration:</p><ul><li>Fix MCS = 9 on both links, yielding a little less than 1 Gbps bidirectional
throughput through DN0</li></ul><p>CN1/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>Default configuration</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN0 to CN1, TC3 Green</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN1, TC3 Yellow</td><td>150</td><td>150, no packet loss</td></tr><tr><td>DN0 to CN2, TC1 Green</td><td>200</td><td>200, no packet loss</td></tr><tr><td>DN0 to CN2, TC3 Green</td><td>200</td><td>200, no packet loss</td></tr><tr><td>DN0 to CN2, TC3 Yellow</td><td>1000</td><td>~250-300</td></tr></tbody></table><p>Notes:</p><ul><li>The intention of this test is to confirm that congestion in the DN0 queue(s)
for CN2 does not cause packet drops in the data path on DN0 for CN1.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2mp-2-p2mp-performance-test-with-cross-traffic"><code>QoS-P2MP-2</code> P2MP performance test with cross traffic<a class="hash-link" href="#qos-p2mp-2-p2mp-performance-test-with-cross-traffic" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2MP</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 12 on both links, yielding approximately 1.8 Gbps bidirectional
throughput capacity (aggregate for the links)<ul><li>Note: ensure we are using Puma sectors that can support MCS = 12.</li></ul></li></ul><p>CN1/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>Default configuration</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN1 to DN0, TC3 Green</td><td>850 with fixed 1500B packets</td><td>~800, Green + Yellow</td></tr><tr><td>CN2 to DN0, TC3 Green</td><td>850 with fixed 1500B packets</td><td>~800, Green + Yellow</td></tr><tr><td>CN1 to CN2, TC0 Green</td><td>100 with fixed 1500B packets</td><td>100, no packet loss</td></tr><tr><td>CN2 to CN1, TC0 Green</td><td>100 with fixed 1500B packets</td><td>100, no packet loss</td></tr><tr><td>DN0 to CN1, TC0 Green</td><td>100 with fixed 1500B packets</td><td>100, no packet loss</td></tr><tr><td>DN0 to CN1, TC3 Green</td><td>850 with fixed 1500B packets</td><td>~700</td></tr><tr><td>DN0 to CN2, TC0 Green</td><td>100 with fixed 1500B packets</td><td>100, no packet loss</td></tr><tr><td>DN0 to CN2, TC3 Green</td><td>850 with fixed 1500B packets</td><td>~700</td></tr></tbody></table><p>Notes:</p><ul><li>The intention of this test is to verify that the high priority traffic
received from CN1 and destined to CN2 is classified and multiplexed properly
on DN0 with the ingress traffic on DN0 destined to CN2. And similarly for
traffic destined to CN1.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-p2mp-3-p2mp-single-sector-full-load-test"><code>QoS-P2MP-3</code> P2MP single sector full load test<a class="hash-link" href="#qos-p2mp-3-p2mp-single-sector-full-load-test" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>P2MP extended to 8 CNs</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 12 on all 8 links, yielding approximately 1.8 Gbps bidirectional
throughput capacity (aggregate for the links, and under the right traffic
conditions)<ul><li>Note: ensure we are using Puma sectors that can support MCS = 12.</li><li>Note 2: this assumes that Puma can support 1.8 Gbps bidirectional in
analogous test conditions without QoS.</li></ul></li></ul><p>CN1/DN1 node configured as CN</p><p>CN policer CIR / EIR configuration:</p><ul><li>Default configuration</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>Each CN to DN0, TC0 Green</td><td>50 with fixed 1500B packets</td><td>50, no packet loss</td></tr><tr><td>Each CN to DN0, TC3 Green</td><td>150 with fixed 1500B packets</td><td>150, no packet loss</td></tr><tr><td>DN0 to each CN, TC0 Green</td><td>100 with fixed 1500B packets</td><td>100, no packet loss</td></tr><tr><td>DN0 to each CN, TC3 Green</td><td>300 with fixed 1500B packets</td><td>~125</td></tr></tbody></table><p>Notes:</p><ul><li>The intention of this test is to verify that for a single sector, the buffer
management on DN0 can scale to handle the parallel load for 8 CNs with a mix
of low and high priority traffic.</li><li>The aggregate UL traffic is 1.6 Gbps in this test, a bit below the
approximate maximum 1.8 Gbps. The intention is to stress the DL data plane and
buffer management, triggering drops due to DL congestion, while also verifying
no loss in the UL receive path.</li><li>If Puma cannot support 1.8 Gbps in a P2MP setup with 8 CNs without QoS, then
for this test we should scale back the aggregate DL traffic accordingly.</li></ul><p><strong>Repeat with TC1 in place of TC0 and TC2 in place of TC3.</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="network-test-cases---nw1">Network Test Cases - NW1<a class="hash-link" href="#network-test-cases---nw1" title="Direct link to heading">​</a></h2><p>For some Network tests, we use the following IF setup, denoted <strong>NW1</strong>:</p><p align="center"><img loading="lazy" src="/terragraph-ctf/figures/QoS-NW1.png" width="800" class="img_ev3q"></p><p>In this network, there are only point-to-point links -- one connected (IF) to
DN0, a second to CN1, and the third to CN2.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-1-policing-enforcement-within-mesh-downstream-traffic"><code>QoS-NW-1</code> Policing enforcement within mesh, downstream traffic<a class="hash-link" href="#qos-nw-1-policing-enforcement-within-mesh-downstream-traffic" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>NW1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 9 on all three links, yielding a little less than 1 Gbps
bidirectional throughput on each link.</li></ul><p>CN policer CIR / EIR configuration:</p><ul><li>Default configuration</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN0 to CN1, TC3 Green</td><td>300</td><td>300, no packet loss</td></tr><tr><td>DN0 to CN2, TC3 Green</td><td>300</td><td>300, no packet loss</td></tr><tr><td>DN0 to CN2, TC3 Yellow</td><td>600</td><td>~400</td></tr></tbody></table><p>Notes:</p><ul><li>Confirm no packet loss on DN1 (nothing specific to QoS functionality).</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-2-policing-enforcement-within-mesh-upstream-traffic"><code>QoS-NW-2</code> Policing enforcement within mesh, upstream traffic<a class="hash-link" href="#qos-nw-2-policing-enforcement-within-mesh-upstream-traffic" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>NW1</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 9 on all three links, yielding a little less than 1 Gbps
bidirectional throughput on each link.</li></ul><p>CN policer CIR / EIR configuration:</p><ul><li>Default configuration</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>CN1 to DN0, TC3 Green</td><td>300</td><td>300, no packet loss</td></tr><tr><td>CN2 to DN0, TC3 Green</td><td>900</td><td>~700, Green + Yellow</td></tr></tbody></table><p>Notes:</p><ul><li>Confirm no packet loss on DN1 (nothing specific to QoS functionality).</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="network-test-cases---nw2">Network Test Cases - NW2<a class="hash-link" href="#network-test-cases---nw2" title="Direct link to heading">​</a></h2><p>For some Network tests, we use the following IF setup, denoted <strong>NW2</strong>:</p><p align="center"><img loading="lazy" src="/terragraph-ctf/figures/QoS-NW2.png" width="800" class="img_ev3q"></p><p>In this network, there are only point-to-point links. DN0 has 4 sectors -- one
connected (IF) to each of 4 other DNs.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-311-aggregate-performance----traffic-profile-1"><code>QoS-NW-3.1.1</code> Aggregate performance -- traffic profile 1<a class="hash-link" href="#qos-nw-311-aggregate-performance----traffic-profile-1" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>NW2</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS to maximum on all 4 links (for DN0: 2 at MCS = 12, 2 at MCS = 11),
yielding approximately 1.8 Gbps bidirectional throughput on each MCS = 12 link
and approximately 1.5 Gbps bidirectional throughput on each MCS=11 link.<ul><li>Note: ensure the MCS = 12 links on DN0 are aligned with links that support
MCS = 12 on the other end.</li></ul></li></ul><p>DN policer configuration:</p><ul><li>N/A (irrelevant -- no end devices attached to GigE ports)</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN1 to DN3, TC3 Green</td><td>1500 with fixed 512B packets</td><td>1500, no packet loss</td></tr><tr><td>DN2 to DN4, TC3 Green</td><td>1500 with fixed 512B packets</td><td>1500, no packet loss</td></tr><tr><td>DN3 to DN2, TC3 Green</td><td>1500 with fixed 512B packets</td><td>1500, no packet loss</td></tr><tr><td>DN4 to DN1, TC3 Green</td><td>1500 with fixed 512B packets</td><td>1500, no packet loss</td></tr></tbody></table><p>Notes:</p><ul><li>This traffic load is consistent with the initial aggregate traffic capability
for Puma.</li><li>From what we understand about the QCOM A-MSDU aggregation logic, using 512B
packets will limit the system to 1.31 Gbps on each link (rather than the 1500
Mbps indicated). This test, <code>QoS-NW-3.1.1</code> is actually not a QoS test — it is
intended to be used as a baseline for later test(s).</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-312-aggregate-performance----traffic-profile-2"><code>QoS-NW-3.1.2</code> Aggregate performance -- traffic profile 2<a class="hash-link" href="#qos-nw-312-aggregate-performance----traffic-profile-2" title="Direct link to heading">​</a></h3><p>Repeat <code>NW-3.1.1</code> with the following changes:</p><ul><li>Send 150 Mbps TC0 Green + 1350 Mbps TC3 Green for each flow.</li><li>Verify no TC0 Green traffic is dropped, and the same total throughput is
delivered to each DN as in <code>QoS-NW-3.1.1</code>.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-313-aggregate-performance----traffic-profile-3"><code>QoS-NW-3.1.3</code> Aggregate performance -- traffic profile 3<a class="hash-link" href="#qos-nw-313-aggregate-performance----traffic-profile-3" title="Direct link to heading">​</a></h3><p>Repeat <code>NW-3.1.1</code> with the following changes:</p><ul><li>Generate traffic using IMIX Model 2. This traffic model is defined above in
this document.</li></ul><p>Note: This traffic mix is consistent with the packet forwarding performance
traffic mix in the PRD. However, it is not explicitly expected to be achievable
for Puma.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-314-aggregate-performance----traffic-profile-4"><code>QoS-NW-3.1.4</code> Aggregate performance -- traffic profile 4<a class="hash-link" href="#qos-nw-314-aggregate-performance----traffic-profile-4" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>NW2</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS to maximum on all 4 links (for DN0: 2 at MCS = 12, 2 at MCS = 11),
yielding approximately 1.8 Gbps bidirectional throughput on each MCS = 12 link
and approximately 1.5 Gbps bidirectional throughput on each MCS=11 link.<ul><li>Note: ensure the MCS = 12 links on DN0 are aligned with links that support
MCS = 12 on the other end.</li></ul></li></ul><p>DN policer configuration:</p><ul><li>N/A (irrelevant -- no end devices attached to GigE ports)</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN1 to DN3, TC3 Green</td><td>1800 with IMIX Model 2</td><td>1800, no packet loss</td></tr><tr><td>DN2 to DN4, TC3 Green</td><td>1500 with IMIX Model 2</td><td>1500, no packet loss</td></tr><tr><td>DN3 to DN1, TC3 Green</td><td>1800 with IMIX Model 2</td><td>1800, no packet loss</td></tr><tr><td>DN4 to DN2, TC3 Green</td><td>1500 with IMIX Model 2</td><td>1500, no packet loss</td></tr></tbody></table><p>Note: This traffic profile is consistent with the final aggregate traffic
capability for Puma based on the PRD. This is not expected to be achievable.
Additionally, based on the A-MSDU aggregation logic within Talyn2, that logic
will also limit us below these PRD rates.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="qos-nw-4-congestion-control-per-wireless-soc"><code>QoS-NW-4</code> Congestion control (per wireless SoC)<a class="hash-link" href="#qos-nw-4-congestion-control-per-wireless-soc" title="Direct link to heading">​</a></h3><p>Physical setup: <strong>NW2</strong></p><p>IF channel configuration:</p><ul><li>Fix MCS = 9 on all four links, yielding approximately 1 Gbps bidirectional
throughput on each link.</li></ul><p>DN policer configuration:</p><ul><li>N/A (irrelevant -- no end devices attached to GigE ports)</li></ul><p>Traffic simultaneous send / verify receive:</p><table><thead><tr><th>Traffic dir and type</th><th>Send (Mbps)</th><th>Verify on receive (Mbps)</th></tr></thead><tbody><tr><td>DN1 to DN4, TC3 Green</td><td>800</td><td>~500</td></tr><tr><td>DN2 to DN4, TC3 Green</td><td>800</td><td>~500</td></tr><tr><td>DN2 to DN1, TC3 Green</td><td>100</td><td>100, no packet loss</td></tr><tr><td>DN3 to DN1, TC3 Green</td><td>800</td><td>800, no packet loss</td></tr></tbody></table><p>Notes:</p><ul><li>This is a multi-hop test. There is a known core data plane packet processing
limitation of approximately 450kPPS for traffic routed from WiGig to WiGig
interfaces. For this test, until this known PPS limitation is improved, we
should run with 800B packets rather than 500B packets. This is the general
recommendation for SIT testing.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/terragraph/terragraph-ctf/edit/main/docs/../docs/tests/QoS.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/terragraph-ctf/tests/Prefix_Allocation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Prefix Allocation (CPA/DPA) Tests</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/terragraph-ctf/tests/Link_Overloading"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Link Overloading Tests</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#data-plane-operation-and-traffic-models" class="table-of-contents__link toc-highlight">Data Plane Operation and Traffic Models</a><ul><li><a href="#user-plane-traffic" class="table-of-contents__link toc-highlight">User Plane Traffic</a></li><li><a href="#policing" class="table-of-contents__link toc-highlight">Policing</a></li></ul></li><li><a href="#testing-defaults-mostly-configuration" class="table-of-contents__link toc-highlight">Testing Defaults (Mostly Configuration)</a><ul><li><a href="#vpp-policer-configuration" class="table-of-contents__link toc-highlight">VPP policer configuration:</a></li><li><a href="#important-note-for-all-test-cases" class="table-of-contents__link toc-highlight">Important Note for All Test Cases</a></li></ul></li><li><a href="#point-to-point-test-cases---p2p1" class="table-of-contents__link toc-highlight">Point to Point Test Cases - P2P1</a><ul><li><a href="#qos-p2p-1-basic-policer-operation----enforce-cir--eir" class="table-of-contents__link toc-highlight"><code>QoS-P2P-1</code> Basic policer operation -- enforce CIR + EIR</a></li><li><a href="#qos-p2p-2-characterize-basic-delay-measurements" class="table-of-contents__link toc-highlight"><code>QoS-P2P-2</code> Characterize basic delay measurements</a></li><li><a href="#qos-p2p-3-characterize-priority-traffic-delay-with-load" class="table-of-contents__link toc-highlight"><code>QoS-P2P-3</code> Characterize priority traffic delay with load</a></li><li><a href="#qos-p2p-4-basic-priority-service-with-congestion-response" class="table-of-contents__link toc-highlight"><code>QoS-P2P-4</code> Basic priority service with congestion response</a></li><li><a href="#qos-p2p-5-basic-priority-service-with-time-varying-congestion-response" class="table-of-contents__link toc-highlight"><code>QoS-P2P-5</code> Basic priority service with time-varying congestion response</a></li><li><a href="#qop2p-6-policer-remarking-with-tcp----confirm-packet-order-preserved" class="table-of-contents__link toc-highlight"><code>QoP2P-6</code> Policer remarking with TCP -- confirm packet order preserved</a></li><li><a href="#qos-p2p-7-four-traffic-classes-with-strict-priority-service" class="table-of-contents__link toc-highlight"><code>QoS-P2P-7</code> Four traffic classes with strict priority service</a></li><li><a href="#qos-p2p-8-handling-unknown-dscps" class="table-of-contents__link toc-highlight"><code>QoS-P2P-8</code> Handling unknown DSCPs</a></li></ul></li><li><a href="#point-to-point-test-cases---p2p2" class="table-of-contents__link toc-highlight">Point to Point Test Cases - P2P2</a><ul><li><a href="#qos-p2p-9-single-link-performance-test" class="table-of-contents__link toc-highlight"><code>QoS-P2P-9</code> Single link performance test</a></li></ul></li><li><a href="#point-to-multipoint-test-cases---p2mp" class="table-of-contents__link toc-highlight">Point to Multipoint Test Cases - P2MP</a><ul><li><a href="#qos-p2mp-1-congestion-control-per-link-per-tc" class="table-of-contents__link toc-highlight"><code>QoS-P2MP-1</code> Congestion control (per link, per TC)</a></li><li><a href="#qos-p2mp-2-p2mp-performance-test-with-cross-traffic" class="table-of-contents__link toc-highlight"><code>QoS-P2MP-2</code> P2MP performance test with cross traffic</a></li><li><a href="#qos-p2mp-3-p2mp-single-sector-full-load-test" class="table-of-contents__link toc-highlight"><code>QoS-P2MP-3</code> P2MP single sector full load test</a></li></ul></li><li><a href="#network-test-cases---nw1" class="table-of-contents__link toc-highlight">Network Test Cases - NW1</a><ul><li><a href="#qos-nw-1-policing-enforcement-within-mesh-downstream-traffic" class="table-of-contents__link toc-highlight"><code>QoS-NW-1</code> Policing enforcement within mesh, downstream traffic</a></li><li><a href="#qos-nw-2-policing-enforcement-within-mesh-upstream-traffic" class="table-of-contents__link toc-highlight"><code>QoS-NW-2</code> Policing enforcement within mesh, upstream traffic</a></li></ul></li><li><a href="#network-test-cases---nw2" class="table-of-contents__link toc-highlight">Network Test Cases - NW2</a><ul><li><a href="#qos-nw-311-aggregate-performance----traffic-profile-1" class="table-of-contents__link toc-highlight"><code>QoS-NW-3.1.1</code> Aggregate performance -- traffic profile 1</a></li><li><a href="#qos-nw-312-aggregate-performance----traffic-profile-2" class="table-of-contents__link toc-highlight"><code>QoS-NW-3.1.2</code> Aggregate performance -- traffic profile 2</a></li><li><a href="#qos-nw-313-aggregate-performance----traffic-profile-3" class="table-of-contents__link toc-highlight"><code>QoS-NW-3.1.3</code> Aggregate performance -- traffic profile 3</a></li><li><a href="#qos-nw-314-aggregate-performance----traffic-profile-4" class="table-of-contents__link toc-highlight"><code>QoS-NW-3.1.4</code> Aggregate performance -- traffic profile 4</a></li><li><a href="#qos-nw-4-congestion-control-per-wireless-soc" class="table-of-contents__link toc-highlight"><code>QoS-NW-4</code> Congestion control (per wireless SoC)</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Terragraph</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://terragraph.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terragraph<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.facebook.com/connectivity/solutions/terragraph" target="_blank" rel="noopener noreferrer" class="footer__link-item">Meta Connectivity<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/terragraph/meta-terragraph" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terragraph<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/terragraph/terragraph-planner" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terragraph Planner<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/terragraph/tgnms" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terragraph NMS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discord.gg/HQaxCevzus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Legal</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terms<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Meta Platforms, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/terragraph-ctf/assets/js/runtime~main.4f2908c1.js"></script>
<script src="/terragraph-ctf/assets/js/main.8da3f691.js"></script>
</body>
</html>